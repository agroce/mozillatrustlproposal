The tool should, by project termination, have basic functionality similar to Python TSTL, albeit with fewer "bells and whistles," in place, be integrated with the Rust toolchain (cargo test, production of a test crate for libraries, etc.), and have a highly tuned method for generating tests that is sensitive to the test budget given by a user.

The metrics for the basic implementation should be clear, in that TSTL is a working system, and can serve as a baseline to compare against.  Additionally, TruSTL should be, by project completion, used to test at least four real, large, Rust projects, and to find a previously unknown fault or faults in at least two of these projects.  Given the success rate for TSTL with Python libraries, this is a reasonable goal, given that Rust projects are likely to have fewer bugs (due to static typing preventing some of the problems found in Python code).

The first major milestone would be translation of a harness file containing pools and test actions into a library that can be used by a simple random test generator, with the ability to save and replay tests.  This is a minimal working version of the tool, and provides substantial benefits:  such a harness can often detect faults that require days or weeks for AFL to detect, due to its knowledge of library semantics encoded by the simple data flow of which actions produce which values for pools.  We would expect, since this step has to resolve the major issue of interaction with the Rust type system, this first milestone to take place during the first four months.

The second milestone would be a release with what we consider minimal features (ones that are not bells and whistles):  working high-quality test reduction and normalization, swarm testing, and export of tests to standalone Rust tests (possibly using a custom test framework).  This release should also provide an untuned implementation of the AFL-TruSTL interface, and ability to use AFL (or another fuzzer) to generate TruSTL tests.

The third milestone would be a tuned system that takes as input a test budget, can run inside a Rust test (so that "run 60 seconds of tuned testing" can be a test like any other performed by "cargo test"), and performs testing optimized for that budget; proof of optimization will be provided by statistically sound experimental comparisons with alternative strategies for a fixed set of popular testing time budgets (e.g. 60 second tests for use during development, 5 minute and 10 minute "coffee break" tests, 30 minute and hour long "lunch break" tests, 12 hour "overnight runs" and 24 and 48 hour "thorough" runs).


