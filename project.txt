In this project we will implement a Rust version of the TSTL (Template Scripting Testing Language) DSL (domain-specific-language) and tool, to be called TruSTL.  TSTL (https://github.com/agroce/tstl) is a Python-based language and toolset for automated test generation.  It is particularly suited for testing imperative, stateful, library code, but has also been used for grammar-based testing of compilers.  Thus far, TSTL has detected (and we have reported) over 150 bugs in systems including: Mac OS X, widely used Python libraries such as sortedcontainers, ESRI's ArcPy, SymPy, and gmpy2, the pyfakefs mock file system, the Solidity solc compiler, unreleased static analysis tools for Solidity developed by Trail of Bits, and the Python implementation itself.  TruSTL would expand on this work to offer a truly powerful platform for making sure we can _trust_ critical software systems that cannot yet be formally verified.

TSTL is (to our knowledge) the first adaptation of many ideas from property-based testing tools like QuickCheck to a more imperative setting.  TSTL tests are sequences of Python actions -- essentially automatically generated unit tests.  TSTL makes it easy to check arbitrarily complex invariants  and post-conditions, perform differential testing, detect unexpected nondeterminism, and implement custom test generation or analysis algorithms.  TSTL also provides unusually strong support for manipulating and understanding complex tests during debugging, including the idea of "normalization" which greatly reduces the problem of automated testing producing many versions of the same fault.  In addition to providing its own test generation tools, using random testing, swarm testing, new heuristics based on code-element-size, and coverage-driven genetics algorithms approaches, TSTL also provides a simple way to use the popular AFL (american fuzzy lop) fuzzer to directly generate TSTL, thus giving AFL the ability to check timing nondeterminism and the other kinds of complex properties TSTL allows, and TSTL access to the complex custom heuristics that have made AFL so successful at finding previously unreported bugs.

The key concept of TSTL is that it (1) embeds a real programming language in its DSL, so that TSTL test actions can be arbitrary code and (2) compiles test harnesses in the TSTL DSL into an interface that hides the details of properties and the Software Under Test (SUT), making it easy to code test generation and manipulation tools: in fact, TSTL can be seen as a tool that, given a description of a testing problems, generates a powerful first-class library for test activities for that SUT.  For more details on TSTL and algorithms provided by TSTL, see these publications, all at https://agroce.github.io: STTT 2018 (https://agroce.github.io/sttt17.pdf), nfm15.pdf, issta15.pdf, issta17.pdf, atest16.pdf, tecps17.pdf, issta17tool.pdf, and interpretableml17.pdf.

The goal of this project is to, from the ground up (but building on the success of the TSTL design and implementation), design and implement a similar, but more powerful, tool for Rust.  A key theme of Rust is the idea that "systems programmers can have nice things."  We believe that automated test generation that treats a library as a first-class entity, makes complex specification easy, and generates readable, concise, idiomatic Rust tests is a very nice thing indeed.  Rust's type system provides many guarantees for critical code, but cannot (any more than the type systems of Haskell or ML) ensure that code actually does what it is intended to do.  Moreover, use of unsafe code within an abstraction boundary always carries the risk of violating a contract.  Formal verification tools and other heavyweight methods may eventually help overcome these challenges, but for the forseeable future, testing is critical if Rust code is to be trustworthy.  Moreover, as our work with the Solidity compiler (fuzzed for years using AFL) shows, TruSTL could be used to test critical software, not written in Rust, as well.  In Python, test generation and execution is painfully slow; with TruSTL, it would be possible to provide an open platform for building extremely efficient, but also reliable, custom test generation tools.

There are a number of fundamental research challenges in this work.  First, the TSTL language embeds and generates Python code.  It uses a template approach to avoid actually parsing the code.  Types in TSTL can be expressed, but simply add a new property to check.  The situation in Rust is very different, and the language will have to be modified to retain the concise, easy-to-read nature of test harnesses while producing Rust testing interfaces with correct lifetimes and ownership of values generated during testing.  One exciting novel approach enabled by Rust is to have TSTL attempt to generate mulitple compiled versions of a harness, trusting the Rust compiler to generate the most efficient and parallelizable version possible.

Second, we will focus on the problem of _effective_ test generation.  The success of AFL has shown that if developers are going to adopt a tool, it must work well out of the box, without difficult tuning and configuration.  Writing test harnesses and properties is hard enough; it is too much to ask that users also tune the generator.  Unfortunately, AFL (and libFuzzer, etc.) all feature heuristics tuned to 1) long runtimes and 2) looking for memory-access violations and crashes  TSTL has algorithms more tuned to the short-duration, frequently-repeated, testing developers expect from a QuickCheck-like tool (TSTL's own tools are much more effective at finding bugs in <10 minute runs than AFL generating TSTL tests), and to the kinds of deep semantic bugs found in Python libraries (and, we expect, Rust code).  However, TSTL is tuned to slow execution and (especially) high cost of obtaining dynamic code coverage information in Python (overheads of 2x-10x are not uncommon).  What are the best algorithms for TruSTL to use, to provide developers immediate, untuned, high-quality testing?  At what length of test run does it become better to use custom tools only to generate an initial set of inputs, and let AFL or another long-term fuzzer take over?  This project will investigate how to automatically, based only on the _time budget allowed for testing_ (and information statically obtainable from the test harness or SUT code) provides highest-quality test generation.

